# ğŸ“š API de Chat com Agente de IA  
### â­ FastAPI + Strands Agents + Ollama (modelo com Tool Use nativo)

Este projeto implementa uma **API de chat** que utiliza o **Strands Agents SDK**, integrado com o **Ollama**, para criar um agente de IA capaz de:

- Responder perguntas de conhecimento geral  
- Detectar quando deve usar ferramentas matemÃ¡ticas  
- Realizar cÃ¡lculos automaticamente usando **LLM com tool-use nativo** (`llama3-groq-tool-use`)  
- Conversar naturalmente com o usuÃ¡rio  

Esse projeto segue as boas prÃ¡ticas solicitadas no case, com organizaÃ§Ã£o, uso de variÃ¡veis de ambiente e separaÃ§Ã£o clara entre API e Agente de IA.

---

# ğŸ—‚ï¸ Estrutura do Projeto


```
API_de_Chat_com_Agente_de_IA/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agent.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ schemas.py
â”‚
â”œâ”€â”€ teste.py
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”§ 1. ConfiguraÃ§Ã£o do Ambiente

O projeto utiliza um arquivo `.env` na raiz para definir suas configuraÃ§Ãµes.

Crie seu `.env`:

```bash
cp .env.example .env
```

O conteÃºdo padrÃ£o:
```
LLM_PROVIDER=ollama
LLM_MODEL=llama3-groq-tool-use

LLM_TEMPERATURE=0.2
LLM_MAX_TOKENS=1024

OLLAMA_HOST=http://localhost:11434

APP_ENV=local
```

LLM_MODEL aponta para o modelo que jÃ¡ possui tool-use nativo

OLLAMA_HOST Ã© o servidor local do Ollama

APP_ENV indica ambiente de execuÃ§Ã£o

---

## ğŸ“¦ 2. DependÃªncias

O arquivo `requirements.txt` contÃ©m:

```
fastapi
uvicorn[standard]
strands-agents
strands-agents-tools
python-dotenv
```

InstalaÃ§Ã£o das dependÃªncias:

```bash
pip install -r requirements.txt
```
---
## ğŸ¤– 3. ImplementaÃ§Ã£o do Agente (Strands Agents + Ollama)

O arquivo `src/agent.py` contÃ©m toda a implementaÃ§Ã£o do agente de IA responsÃ¡vel por interpretar a mensagem do usuÃ¡rio e gerar a resposta final utilizando o Strands Agents e o Ollama.

### âœ”ï¸ Carregamento do modelo Ollama

O agente utiliza o `OllamaModel`, configurado atravÃ©s das variÃ¡veis definidas no arquivo `.env`.  
Essas configuraÃ§Ãµes incluem:

- `LLM_MODEL`: nome do modelo  
- `LLM_TEMPERATURE`: temperatura do modelo  
- `LLM_MAX_TOKENS`: mÃ¡ximo de tokens de resposta  
- `OLLAMA_HOST`: endereÃ§o do servidor Ollama  

Isso permite que o comportamento do modelo seja facilmente ajustado sem alterar o cÃ³digo.

### âœ”ï¸ Tool-Use nativo do modelo

O projeto utiliza o modelo **`llama3-groq-tool-use`**, que jÃ¡ possui suporte nativo para tool-use, incluindo:

- execuÃ§Ã£o de cÃ¡lculos matemÃ¡ticos  
- raciocÃ­nio estruturado  
- seleÃ§Ã£o automÃ¡tica de ferramentas internas  
- interpretaÃ§Ã£o inteligente de comandos  

Nenhuma tool foi implementada manualmente no Python.  
O prÃ³prio modelo decide quando usar uma ferramenta ou quando responder diretamente.

### âœ”ï¸ System Prompt

O agente possui um *system prompt* que orienta o modelo quanto ao comportamento esperado, incluindo:

- quando usar tool-use  
- como responder perguntas gerais  
- como manter coerÃªncia e linguagem adequada  
- manter o idioma da resposta igual ao input do usuÃ¡rio  

Esse prompt garante consistÃªncia nas respostas e melhora a qualidade da interaÃ§Ã£o.

### âœ”ï¸ FunÃ§Ã£o `run_agent(message)`

A funÃ§Ã£o `run_agent` Ã© utilizada tanto pela API quanto pelo arquivo `teste.py`.  
Ela executa o fluxo:

1. Recebe a mensagem do usuÃ¡rio  
2. Envia essa mensagem ao agente Strands  
3. Aguarda o processamento do modelo via Ollama  
4. Retorna a resposta final formatada como string  

Essa funÃ§Ã£o centraliza a execuÃ§Ã£o do modelo e facilita a reutilizaÃ§Ã£o do agente em vÃ¡rias partes do projeto.

---

## ğŸŒ 4. API FastAPI (src/main.py)

A API expÃµe dois endpoints:

### POST /chat

**Entrada:**

    {
      "message": "Quanto Ã© 1234 * 5678?"
    }

**SaÃ­da:**

    {
      "response": "7006652"
    }

### GET /health

**Resposta:**

    {
      "status": "ok"
    }

---

## ğŸ§  5. IntegraÃ§Ã£o com o Ollama

Este projeto usa o **Ollama** para rodar o modelo localmente.

### 5.1 InstalaÃ§Ã£o

    curl -fsSL https://ollama.com/install.sh | sh

Verifique a instalaÃ§Ã£o:

    ollama -v

### 5.2 Baixar o modelo requerido

    ollama pull llama3-groq-tool-use

Listar modelos:

    ollama list

### 5.3 Servidor do Ollama

Inicie o servidor do Ollama:

    ollama serve

---

## â–¶ï¸ Como Rodar

Subir a API FastAPI:

    uvicorn src.main:app --reload

DocumentaÃ§Ã£o:

    http://127.0.0.1:8000/docs

---

## ğŸ§ª Testando o Agente

Executar o script de teste:

    python teste.py
---
## ğŸ§ª 6. Testes e ValidaÃ§Ã£o

Esta fase garante que o agente, a API e a integraÃ§Ã£o com o Ollama estÃ£o funcionando corretamente.

---

### 6.1 Testando o Agente diretamente (teste.py)

Execute:

    python teste.py

Resultados esperados:

- Pergunta matemÃ¡tica:
    Input:
        Quanto Ã© 1234 * 5678?
    Output esperado:
        7006652

- Raiz quadrada:
    Input:
        Qual a raiz quadrada de 144?
    Output esperado:
        12

- Pergunta geral:
    Input:
        Quem foi Ada Lovelace?
    Output esperado:
        Uma explicaÃ§Ã£o descritiva.

---

### 6.2 Testando a API via Swagger

Acesse:

    http://localhost:8000/docs

Teste o endpoint POST /chat:

Entrada:

    {
      "message": "Qual a raiz quadrada de 144?"
    }

SaÃ­da esperada:

    {
      "response": "12"
    }

Teste pergunta geral:

Entrada:

    {
      "message": "Explique o que Ã© machine learning."
    }

---

### 6.3 Testando via cURL

Teste de cÃ¡lculo:

    curl -X POST http://localhost:8000/chat \
    -H "Content-Type: application/json" \
    -d '{"message": "Quanto Ã© 55 * 99?"}'

Teste pergunta geral:

    curl -X POST http://localhost:8000/chat \
    -H "Content-Type: application/json" \
    -d '{"message": "Quem inventou o aviÃ£o?"}'

---

### 6.4 Verificando o Ollama

Listar modelos:

    ollama list

Testar diretamente o modelo:

    ollama run llama3-groq-tool-use "Quanto Ã© 120 * 88?"

---

### 6.5 Casos de teste recomendados (tool-use)

    Quanto Ã© 8 ** 3?
    Raiz quadrada de 256.
    Calcule 55 * 45.
    Quanto Ã© 0.55 * 1200?

---

### 6.6 Casos de teste recomendados (conhecimento geral)

    Quem foi Albert Einstein?
    Explique redes neurais.
    O que Ã© Python?
    Explique o conceito de API.

---

### 6.7 Resultado da Fase 6

- O agente resolve cÃ¡lculos corretamente.  
- O agente responde perguntas gerais de forma coerente.  
- O endpoint POST /chat funciona via Swagger e cURL.  
- O modelo llama3-groq-tool-use estÃ¡ operando corretamente no Ollama.  
- Toda a aplicaÃ§Ã£o estÃ¡ validada e funcional.



---

## ğŸ‰ Fases concluÃ­das

- âœ”ï¸ Fase 1 â€” Estrutura  
- âœ”ï¸ Fase 2 â€” ConfiguraÃ§Ã£o  
- âœ”ï¸ Fase 3 â€” DependÃªncias  
- âœ”ï¸ Fase 4 â€” API FastAPI  
- âœ”ï¸ Fase 5 â€” IntegraÃ§Ã£o com Ollama
- âœ”ï¸ Fase 6 â€” Testes e ValidaÃ§Ã£o


---

