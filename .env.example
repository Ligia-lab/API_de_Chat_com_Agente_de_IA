# Provider do modelo de linguagem (LLM)
LLM_PROVIDER=ollama

# Nome do modelo no Ollama
LLM_MODEL=llama3-groq-tool-use

# Parâmetros do modelo
LLM_TEMPERATURE=0.2

# Máximo de tokens na resposta
LLM_MAX_TOKENS=1024

# Ambiente da aplicação
APP_ENV=local

# Host do servidor do Ollama (porta padrão é 11434)
LLM_API_BASE=http://localhost:11434


